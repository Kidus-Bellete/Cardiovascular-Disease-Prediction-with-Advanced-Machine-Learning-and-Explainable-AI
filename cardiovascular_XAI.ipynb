{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1TCvyyXTWB3ZVsPHdZiPp22zulrSBCWqr",
      "authorship_tag": "ABX9TyMBoP7Gnwt4uZPGFrOrVnpU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kidus-Bellete/Cardiovascular-Disease-Prediction-with-Advanced-Machine-Learning-and-Explainable-AI/blob/main/cardiovascular_XAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install shap\n",
        "#!pip install imblearn"
      ],
      "metadata": {
        "id": "FMQ4amdm1FHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Necessary Libraries**"
      ],
      "metadata": {
        "id": "T_OsAJEAcTZS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6SZco3r1A9m"
      },
      "outputs": [],
      "source": [
        "# Basic data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Deep learning with TensorFlow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# Model evaluation metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "# Explainable AI\n",
        "import shap\n",
        "\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/XAI/heart_failure_clinical_records_dataset-1-1.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "rc_7Lh531ukv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "print(\"the total number of missing value in each features:\")\n",
        "print(df.isnull().sum())\n",
        "df_duplicated = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {df_duplicated}\")\n",
        "df = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "Jc247Q2E4LX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "gmIQjLx82MXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T"
      ],
      "metadata": {
        "id": "zHxDMDw-C1fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap\n",
        "plt.figure(figsize=(14, 10))\n",
        "correlation_matrix = df.corr()\n",
        "mask = np.triu(correlation_matrix)\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
        "            mask=mask, linewidths=0.5, vmin=-1, vmax=1)\n",
        "plt.title('Correlation Heatmap of Heart Failure Features', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gy624dtjC5EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation with Target Variable (DEATH_EVENT)\n",
        "plt.figure(figsize=(8, 6))\n",
        "death_correlation = correlation_matrix['DEATH_EVENT'].drop('DEATH_EVENT').sort_values(ascending=False)\n",
        "sns.barplot(x=death_correlation.values, y=death_correlation.index, hue=death_correlation.index, palette='viridis', legend=False)\n",
        "plt.title('Correlation of Features with DEATH_EVENT', fontsize=16)\n",
        "plt.xlabel('Correlation Coefficient')\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6oRjLHs5C_Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Numerical Features by Death Event\n",
        "numerical_features = ['age', 'creatinine_phosphokinase', 'ejection_fraction',\n",
        "                      'platelets', 'serum_creatinine', 'serum_sodium', 'time']\n",
        "\n",
        "fig, axes = plt.subplots(6, 3, figsize=(10, 20))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(numerical_features):\n",
        "    if i < len(axes):\n",
        "        sns.histplot(data=df, x=feature, hue='DEATH_EVENT', kde=True, bins=25,\n",
        "                    palette={0: 'green', 1: 'red'}, alpha=0.6, ax=axes[i])\n",
        "        axes[i].set_title(f'Distribution of {feature} by Death Event', fontsize=14)\n",
        "        axes[i].set_xlabel(feature, fontsize=12)\n",
        "        axes[i].set_ylabel('Count', fontsize=12)\n",
        "        axes[i].legend(['Survived', 'Died'], title='Outcome')\n",
        "        axes[i].grid(linestyle='--', alpha=0.7)\n",
        "\n",
        "# Remove any unused subplot\n",
        "for i in range(len(numerical_features), len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rDh9iNO6DFWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplots for Key Features by Death Event\n",
        "key_features = ['age', 'ejection_fraction', 'serum_creatinine', 'time', 'serum_sodium']\n",
        "\n",
        "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(key_features):\n",
        "    if i < len(axes):\n",
        "        sns.boxplot(x='DEATH_EVENT', y=feature, data=df,\n",
        "                   hue='DEATH_EVENT', palette={0: 'green', 1: 'red'},\n",
        "                   legend=False, ax=axes[i])\n",
        "        axes[i].set_title(f'Distribution of {feature} by Death Event', fontsize=14)\n",
        "        axes[i].set_xlabel('Death Event (0=No, 1=Yes)', fontsize=12)\n",
        "        axes[i].set_ylabel(feature, fontsize=12)\n",
        "        axes[i].grid(linestyle='--', alpha=0.7)\n",
        "\n",
        "# Remove any unused subplot\n",
        "for i in range(len(key_features), len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X1f-q8sQDb9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical Features Analysis\n",
        "categorical_features = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']\n",
        "\n",
        "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(categorical_features):\n",
        "    if i < len(axes):\n",
        "        # Create a cross-tabulation\n",
        "        cross_tab = pd.crosstab(df[feature], df['DEATH_EVENT'])\n",
        "        cross_tab_percentage = cross_tab.div(cross_tab.sum(axis=1), axis=0) * 100\n",
        "\n",
        "        # Plot stacked bar chart\n",
        "        cross_tab_percentage.plot(kind='bar', stacked=True,\n",
        "                                colormap='viridis', ax=axes[i], rot=0)\n",
        "        axes[i].set_title(f'Death Rate by {feature}', fontsize=14)\n",
        "        axes[i].set_xlabel(f'{feature} (0=No, 1=Yes)', fontsize=12)\n",
        "        axes[i].set_ylabel('Percentage (%)', fontsize=12)\n",
        "        axes[i].legend(['Survived', 'Died'], title='Outcome')\n",
        "        axes[i].grid(linestyle='--', alpha=0.7)\n",
        "\n",
        "        # Add percentage labels\n",
        "        for j, p in enumerate(axes[i].patches):\n",
        "            width, height = p.get_width(), p.get_height()\n",
        "            x, y = p.get_xy()\n",
        "            if height > 5:  # Only label if percentage is greater than 5%\n",
        "                axes[i].text(x + width/2, y + height/2, f'{height:.1f}%',\n",
        "                            ha='center', va='center')\n",
        "\n",
        "# Remove any unused subplot\n",
        "for i in range(len(categorical_features), len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YJN3hffpEFm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pairplot of Most Correlated Features\n",
        "most_correlated_features = ['time', 'serum_creatinine', 'ejection_fraction', 'age', 'DEATH_EVENT']\n",
        "sns.pairplot(df[most_correlated_features], hue='DEATH_EVENT',\n",
        "             palette={0: 'green', 1: 'red'}, corner=True)\n",
        "plt.suptitle('Pairplot of Most Correlated Features', y=1.02, fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4RpBfuytEZi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Age vs Time with Death Event\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(data=df, x='age', y='time', hue='DEATH_EVENT',\n",
        "               palette={0: 'green', 1: 'red'}, size='serum_creatinine',\n",
        "               sizes=(20, 200), alpha=0.7)\n",
        "plt.title('Age vs. Follow-up Time by Death Event', fontsize=16)\n",
        "plt.xlabel('Age', fontsize=14)\n",
        "plt.ylabel('Follow-up Time (days)', fontsize=14)\n",
        "plt.grid(linestyle='--', alpha=0.7)\n",
        "plt.legend(title='Death Event', labels=['Survived', 'Died'])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u-LiBn5tFdh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of all continuous variables\n",
        "continuous_variables = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets',\n",
        "                   'serum_creatinine', 'serum_sodium', 'time']\n",
        "# Standared Scaler\n",
        "scaler = StandardScaler()\n",
        "df[continuous_variables] = scaler.fit_transform(df[continuous_variables])\n",
        "\n",
        "#MnMax Scaler\n",
        "# minmax_scaler = MinMaxScaler()\n",
        "# df[continuous_variables] = minmax_scaler.fit_transform(df[continuous_variables])\n",
        "\n",
        "#Robust Scaler\n",
        "# robust_scaler = RobustScaler()\n",
        "# df[continuous_variables] = robust_scaler.fit_transform(df[continuous_variables])"
      ],
      "metadata": {
        "id": "rze43WQoHnPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "# Define features (x) and target (y)\n",
        "x = df.iloc[:, :-1].values  # All columns except the last one (DEATH_EVENT)\n",
        "y = df.iloc[:, -1].values   # The last column (DEATH_EVENT)\n",
        "\n",
        "# Check class distribution before SMOTE\n",
        "counter_before = Counter(y)\n",
        "print(f'Class distribution before SMOTE: {counter_before}')\n",
        "\n",
        "# Apply SMOTE to balance the classes in DEATH_EVENT\n",
        "oversample = SMOTE()\n",
        "x, y = oversample.fit_resample(x, y)\n",
        "\n",
        "# Check class distribution after SMOTE\n",
        "counter_after = Counter(y)\n",
        "print(f'Class distribution after SMOTE: {counter_after}')\n",
        "\n",
        "# Split the data into training (70%) , val(15%) and tset(15%)\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.30, random_state=42)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.50, random_state=42)\n",
        "\n",
        "# Print the shapes of the datasets\n",
        "print(f\"Training set shape: {x_train.shape}\")\n",
        "print(f\"Validation set shape: {x_val.shape}\")\n",
        "print(f\"Test set shape: {x_test.shape}\")"
      ],
      "metadata": {
        "id": "IOXaZve3JebH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Models (Logistic Regression, Random Forest)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                            f1_score, roc_auc_score, classification_report)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Standardize data for Logistic Regression\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_val_scaled = scaler.transform(x_val)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "def evaluate_model(model, model_name, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation function for ML models\n",
        "    \"\"\"\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Generate predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred),\n",
        "        'Recall': recall_score(y_test, y_pred),\n",
        "        'F1': f1_score(y_test, y_pred),\n",
        "        'ROC AUC': roc_auc_score(y_test, y_prob) if y_prob is not None else None\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\"{model_name} Evaluation Results\")\n",
        "    print('='*40)\n",
        "    print(classification_report(y_test, y_pred, target_names=['No Death', 'Death']))\n",
        "\n",
        "    for metric, value in metrics.items():\n",
        "        if value is not None:\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# =============================================\n",
        "# Logistic Regression\n",
        "# =============================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LOGISTIC REGRESSION MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logreg_metrics = evaluate_model(\n",
        "    logreg,\n",
        "    \"Logistic Regression\",\n",
        "    x_train_scaled, y_train,\n",
        "    x_test_scaled, y_test\n",
        ")\n",
        "\n",
        "# =============================================\n",
        "# Random Forest\n",
        "# =============================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RANDOM FOREST MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_metrics = evaluate_model(\n",
        "    rf_model,\n",
        "    \"Random Forest\",\n",
        "    x_train, y_train,  # Random Forest doesn't require scaling\n",
        "    x_test, y_test\n",
        ")\n",
        "\n",
        "# Feature importance visualization\n",
        "X = df.drop('DEATH_EVENT', axis=1)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=feature_importance.head(15), x='importance', y='feature')\n",
        "plt.title('Top 15 Feature Importances (Random Forest)')\n",
        "plt.xlabel('Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance.head(10).to_string(index=False))\n",
        "\n",
        "# =============================================\n",
        "# Model Comparison\n",
        "# =============================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_data = {\n",
        "    'Model': ['Logistic Regression', 'Random Forest'],\n",
        "    'Accuracy': [\n",
        "        logreg_metrics['Accuracy'],\n",
        "        rf_metrics['Accuracy']\n",
        "    ],\n",
        "    'Precision': [\n",
        "        logreg_metrics['Precision'],\n",
        "        rf_metrics['Precision']\n",
        "    ],\n",
        "    'Recall': [\n",
        "        logreg_metrics['Recall'],\n",
        "        rf_metrics['Recall']\n",
        "    ],\n",
        "    'F1 Score': [\n",
        "        logreg_metrics['F1'],\n",
        "        rf_metrics['F1']\n",
        "    ],\n",
        "    'ROC AUC': [\n",
        "        logreg_metrics['ROC AUC'],\n",
        "        rf_metrics['ROC AUC']\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "print(comparison_df.round(4).to_string(index=False))"
      ],
      "metadata": {
        "id": "TOwcnAnjBFH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Notebook\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Main Network Architecture\n",
        "def build_simple_nn(input_dim, architecture='medium'):\n",
        "    model = Sequential()\n",
        "\n",
        "    if architecture == 'small':\n",
        "        # Small architecture\n",
        "        model.add(Dense(32, activation='relu', input_shape=(input_dim,)))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(16, activation='relu'))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    elif architecture == 'medium':\n",
        "        # Medium architecture\n",
        "        model.add(Dense(64, activation='relu', input_shape=(input_dim,)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Dense(16, activation='relu'))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    elif architecture == 'large':\n",
        "        # Larger architecture\n",
        "        model.add(Dense(128, activation='relu', input_shape=(input_dim,)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Dense(16, activation='relu'))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile model\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['accuracy',\n",
        "                         tf.keras.metrics.AUC(name='auc'),\n",
        "                         tf.keras.metrics.Precision(name='precision'),\n",
        "                         tf.keras.metrics.Recall(name='recall')])\n",
        "    return model\n",
        "\n",
        "def build_cnn_model(input_dim):\n",
        "    \"\"\"\n",
        "    Build a 1D CNN model for tabular data\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Reshape input for 1D CNN (samples, features, 1)\n",
        "    model.add(tf.keras.layers.Reshape((input_dim, 1), input_shape=(input_dim,)))\n",
        "\n",
        "    # First Conv1D block\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(MaxPool1D(pool_size=2))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Second Conv1D block\n",
        "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(MaxPool1D(pool_size=2))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Third Conv1D block\n",
        "    model.add(Conv1D(filters=16, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Flatten and dense layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile model\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['accuracy',\n",
        "                         tf.keras.metrics.AUC(name='auc'),\n",
        "                         tf.keras.metrics.Precision(name='precision'),\n",
        "                         tf.keras.metrics.Recall(name='recall')])\n",
        "    return model\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "# Train models with different architectures\n",
        "architectures = ['small', 'medium', 'large', 'cnn']\n",
        "models = {}\n",
        "histories = {}\n",
        "weighted_models = {}  # New dictionary for weighted models\n",
        "weighted_histories = {}  # New dictionary for weighted histories\n",
        "\n",
        "for arch in architectures:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training {arch.upper()} {'CNN' if arch == 'cnn' else 'Neural Network'}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Build model\n",
        "    if arch == 'cnn':\n",
        "        model = build_cnn_model(x_train.shape[1])\n",
        "    else:\n",
        "        model = build_simple_nn(x_train.shape[1], arch)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_auc', patience=20, mode='max', restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n",
        "    ]\n",
        "\n",
        "    # Train standard model\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        validation_data=(x_val, y_val),\n",
        "        epochs=50,\n",
        "        batch_size=16,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    models[arch] = model\n",
        "    histories[arch] = history\n",
        "\n",
        "    # Train weighted version (except for CNN)\n",
        "    if arch != 'cnn':\n",
        "        print(f\"\\nTraining WEIGHTED {arch.upper()} Neural Network\")\n",
        "        weighted_model = build_simple_nn(x_train.shape[1], arch)\n",
        "\n",
        "        weighted_history = weighted_model.fit(\n",
        "            x_train, y_train,\n",
        "            validation_data=(x_val, y_val),\n",
        "            epochs=50,\n",
        "            batch_size=16,\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weight_dict,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        weighted_models[f'{arch}_weighted'] = weighted_model\n",
        "        weighted_histories[f'{arch}_weighted'] = weighted_history"
      ],
      "metadata": {
        "id": "FxhOwguiFTNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (roc_curve, roc_auc_score, classification_report,\n",
        "                            confusion_matrix, precision_recall_curve, average_precision_score)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# =============================================\n",
        "# 1. Evaluation Functions\n",
        "# =============================================\n",
        "\n",
        "def evaluate_ml_model(model, X, y, model_name, model_type=\"ML\"):\n",
        "    \"\"\"\n",
        "    Evaluate traditional ML models (Logistic Regression, SVM, Random Forest)\n",
        "    \"\"\"\n",
        "    y_prob = model.predict_proba(X)[:, 1]\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # Metrics\n",
        "    metrics = {\n",
        "        'Accuracy': accuracy_score(y, y_pred),\n",
        "        'Precision': precision_score(y, y_pred),\n",
        "        'Recall': recall_score(y, y_pred),\n",
        "        'F1': f1_score(y, y_pred),\n",
        "        'ROC AUC': roc_auc_score(y, y_prob),\n",
        "        'PR AUC': average_precision_score(y, y_prob)\n",
        "    }\n",
        "\n",
        "    # Print report\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{model_name} ({model_type}) Evaluation\")\n",
        "    print('='*60)\n",
        "    print(classification_report(y, y_pred, target_names=['No Death', 'Death']))\n",
        "\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    return metrics, y_prob, y_pred\n",
        "\n",
        "def evaluate_nn_model(model, X, y, model_name):\n",
        "    \"\"\"\n",
        "    Evaluate neural network models\n",
        "    \"\"\"\n",
        "    y_prob = model.predict(X, verbose=0).flatten()\n",
        "    y_pred = (y_prob > 0.5).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    metrics = {\n",
        "        'Accuracy': accuracy_score(y, y_pred),\n",
        "        'Precision': precision_score(y, y_pred),\n",
        "        'Recall': recall_score(y, y_pred),\n",
        "        'F1': f1_score(y, y_pred),\n",
        "        'ROC AUC': roc_auc_score(y, y_prob),\n",
        "        'PR AUC': average_precision_score(y, y_prob)\n",
        "    }\n",
        "\n",
        "    # Print report\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{model_name} (Neural Network) Evaluation\")\n",
        "    print('='*60)\n",
        "    print(classification_report(y, y_pred, target_names=['No Death', 'Death']))\n",
        "\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    return metrics, y_prob, y_pred\n",
        "\n",
        "# =============================================\n",
        "# 2. Visualization Functions\n",
        "# =============================================\n",
        "\n",
        "# def plot_confusion_matrix(y_true, y_pred, model_name):\n",
        "#     cm = confusion_matrix(y_true, y_pred)\n",
        "#     plt.figure(figsize=(6, 6))\n",
        "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "#                 xticklabels=['No Death', 'Death'],\n",
        "#                 yticklabels=['No Death', 'Death'])\n",
        "#     plt.title(f'{model_name} - Confusion Matrix')\n",
        "#     plt.ylabel('True label')\n",
        "#     plt.xlabel('Predicted label')\n",
        "#     plt.show()\n",
        "\n",
        "def plot_roc_curves(results_dict):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for name, data in results_dict.items():\n",
        "        fpr, tpr, _ = roc_curve(data['y_true'], data['y_prob'])\n",
        "        auc_score = roc_auc_score(data['y_true'], data['y_prob'])\n",
        "        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})', linewidth=2)\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves Comparison')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "def plot_pr_curves(results_dict):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for name, data in results_dict.items():\n",
        "        precision, recall, _ = precision_recall_curve(data['y_true'], data['y_prob'])\n",
        "        ap_score = average_precision_score(data['y_true'], data['y_prob'])\n",
        "        plt.plot(recall, precision, label=f'{name} (AP = {ap_score:.3f})', linewidth=2)\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curves')\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "def plot_metric_comparison(comparison_df):\n",
        "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC', 'PR AUC']\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, metric in enumerate(metrics, 1):\n",
        "        plt.subplot(2, 3, i)\n",
        "        sns.barplot(data=comparison_df, x=metric, y='Model')\n",
        "        plt.title(metric)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# =============================================\n",
        "# 3. Main Evaluation Section\n",
        "# =============================================\n",
        "\n",
        "# Initialize storage for results\n",
        "all_results = {}\n",
        "comparison_data = []\n",
        "\n",
        "# Evaluate ML Models (assuming they're trained in Notebook 1)\n",
        "ml_models = {\n",
        "    'Logistic Regression': logreg,\n",
        "    'Random Forest': rf_model\n",
        "}\n",
        "\n",
        "for name, model in ml_models.items():\n",
        "    metrics, y_prob, y_pred = evaluate_ml_model(model, x_test, y_test, name)\n",
        "    all_results[name] = {'y_true': y_test, 'y_prob': y_prob, 'y_pred': y_pred}\n",
        "    comparison_data.append({'Model': name, **metrics})\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    #plot_confusion_matrix(y_test, y_pred, name)\n",
        "\n",
        "# Evaluate Neural Networks (assuming they're trained in Notebook 2)\n",
        "nn_models = {\n",
        "    'Small NN': models['small'],\n",
        "    'Medium NN': models['medium'],\n",
        "    'Large NN': models['large'],\n",
        "    'CNN': models['cnn']\n",
        "}\n",
        "\n",
        "for name, model in nn_models.items():\n",
        "    metrics, y_prob, y_pred = evaluate_nn_model(model, x_test, y_test, name)\n",
        "    all_results[name] = {'y_true': y_test, 'y_prob': y_prob, 'y_pred': y_pred}\n",
        "    comparison_data.append({'Model': name, **metrics})\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    #plot_confusion_matrix(y_test, y_pred, name)\n",
        "\n",
        "# Evaluate Weighted Neural Networks (if available)\n",
        "if 'weighted_models' in globals():\n",
        "    weighted_nn_models = {\n",
        "        'Small NN (Weighted)': weighted_models['small_weighted'],\n",
        "        'Medium NN (Weighted)': weighted_models['medium_weighted'],\n",
        "        'Large NN (Weighted)': weighted_models['large_weighted']\n",
        "    }\n",
        "\n",
        "    for name, model in weighted_nn_models.items():\n",
        "        metrics, y_prob, y_pred = evaluate_nn_model(model, x_test, y_test, name)\n",
        "        all_results[name] = {'y_true': y_test, 'y_prob': y_prob, 'y_pred': y_pred}\n",
        "        comparison_data.append({'Model': name, **metrics})\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        #plot_confusion_matrix(y_test, y_pred, name)\n",
        "\n",
        "# =============================================\n",
        "# 4. Comprehensive Comparison\n",
        "# =============================================\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Display comparison table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.round(4).to_string(index=False))\n",
        "\n",
        "# Visual comparisons\n",
        "#plot_metric_comparison(comparison_df)\n",
        "#plot_roc_curves(all_results)\n",
        "#plot_pr_curves(all_results)\n",
        "\n",
        "# =============================================\n",
        "# 5. Neural Network Training Analysis\n",
        "# =============================================\n",
        "\n",
        "if 'histories' in globals():\n",
        "    # Plot training histories for neural networks\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "    fig.suptitle('Neural Network Training Histories', fontsize=16)\n",
        "\n",
        "    for i, (arch, history) in enumerate(histories.items()):\n",
        "        # AUC plot\n",
        "        axes[0, i].plot(history.history['auc'], label='Train AUC', alpha=0.8)\n",
        "        axes[0, i].plot(history.history['val_auc'], label='Validation AUC', alpha=0.8)\n",
        "        model_name = 'CNN' if arch == 'cnn' else f'{arch.capitalize()} NN'\n",
        "        axes[0, i].set_title(f'{model_name} - AUC')\n",
        "        axes[0, i].set_ylabel('AUC')\n",
        "        axes[0, i].set_xlabel('Epoch')\n",
        "        axes[0, i].legend()\n",
        "        axes[0, i].grid(True, alpha=0.3)\n",
        "\n",
        "        # Loss plot\n",
        "        axes[1, i].plot(history.history['loss'], label='Train Loss', alpha=0.8)\n",
        "        axes[1, i].plot(history.history['val_loss'], label='Validation Loss', alpha=0.8)\n",
        "        axes[1, i].set_title(f'{model_name} - Loss')\n",
        "        axes[1, i].set_ylabel('Loss')\n",
        "        axes[1, i].set_xlabel('Epoch')\n",
        "        axes[1, i].legend()\n",
        "        axes[1, i].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yKtDWh4FFUv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install lime"
      ],
      "metadata": {
        "id": "U81EyNWz-9cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XAI Analysis - Permutation + SHAP + LIME for ALL Models Including Neural Networks\n",
        "import shap\n",
        "from lime import lime_tabular\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"COMPLETE XAI ANALYSIS - SHAP + LIME + PERMUTATION FOR ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# INITIALIZATION AND DATA PREP\n",
        "# =============================================\n",
        "\n",
        "# Get feature names from the original dataframe\n",
        "feature_names = [col for col in df.columns if col != 'DEATH_EVENT']\n",
        "print(f\"Feature names: {feature_names}\")\n",
        "print(f\"Number of features: {len(feature_names)}\")\n",
        "\n",
        "# Define all models to analyze\n",
        "all_models = {\n",
        "    'small_nn': {'model': models['small'], 'type': 'nn', 'name': 'Small Neural Network'},\n",
        "    'medium_nn': {'model': models['medium'], 'type': 'nn', 'name': 'Medium Neural Network'},\n",
        "    'large_nn': {'model': models['large'], 'type': 'nn', 'name': 'Large Neural Network'},\n",
        "    'cnn': {'model': models['cnn'], 'type': 'nn', 'name': 'CNN'},\n",
        "    'random_forest': {'model': rf_model, 'type': 'tree', 'name': 'Random Forest'},\n",
        "    'logistic_regression': {'model': logreg, 'type': 'linear', 'name': 'Logistic Regression'}\n",
        "}\n",
        "\n",
        "# PERMUTATION IMPORTANCE\n",
        "# =============================================\n",
        "\n",
        "def calculate_permutation_importance(model, X, y, feature_names, model_type=\"nn\", model_name=\"Model\"):\n",
        "    \"\"\"Calculate permutation importance for any model type\"\"\"\n",
        "    print(f\"\\nCalculating Permutation Importance - {model_name}\")\n",
        "\n",
        "    try:\n",
        "        # Create prediction function based on model type\n",
        "        if model_type == \"nn\":\n",
        "            def predict_fn(X):\n",
        "                return model.predict(X, verbose=0).flatten()\n",
        "        else:\n",
        "            def predict_fn(X):\n",
        "                return model.predict_proba(X)[:, 1]\n",
        "\n",
        "        # Calculate baseline score\n",
        "        baseline_pred = predict_fn(X)\n",
        "        baseline_score = roc_auc_score(y, baseline_pred)\n",
        "        print(f\"   Baseline ROC-AUC: {baseline_score:.4f}\")\n",
        "\n",
        "        # Calculate permutation importance with multiple repetitions for stability\n",
        "        importances = []\n",
        "        n_repeats = 3  # Reduced for speed but still stable\n",
        "\n",
        "        for i, feature in enumerate(feature_names):\n",
        "            feature_importances = []\n",
        "            for _ in range(n_repeats):\n",
        "                X_permuted = X.copy()\n",
        "                np.random.shuffle(X_permuted[:, i])\n",
        "                permuted_pred = predict_fn(X_permuted)\n",
        "                permuted_score = roc_auc_score(y, permuted_pred)\n",
        "                feature_importances.append(baseline_score - permuted_score)\n",
        "            importances.append(np.mean(feature_importances))\n",
        "\n",
        "        # Create importance dataframe\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': importances\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        print(f\"  Top 3 features: {', '.join(importance_df.head(3)['feature'].tolist())}\")\n",
        "        return importance_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   Error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# UNIVERSAL SHAP ANALYSIS\n",
        "# =============================================\n",
        "\n",
        "def perform_universal_shap_analysis(model, X, feature_names, model_type=\"nn\", model_name=\"Model\"):\n",
        "    \"\"\"Perform SHAP analysis for ANY model type including neural networks\"\"\"\n",
        "    print(f\"\\nSHAP Analysis - {model_name}\")\n",
        "\n",
        "    try:\n",
        "        # Use appropriate sample sizes for computational efficiency\n",
        "        if model_type == \"nn\":\n",
        "            # Smaller samples for neural networks due to computational cost\n",
        "            n_background = 50\n",
        "            n_explain = 30\n",
        "        else:\n",
        "            # Larger samples for traditional ML models\n",
        "            n_background = 100\n",
        "            n_explain = 50\n",
        "\n",
        "        X_background = X[:n_background]\n",
        "        X_explain = X[:n_explain]\n",
        "\n",
        "        if model_type == \"tree\":\n",
        "            # TreeExplainer for Random Forest (fastest)\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "            shap_values = explainer.shap_values(X_explain)\n",
        "            if isinstance(shap_values, list):\n",
        "                shap_values = shap_values[1]  # Binary classification positive class\n",
        "\n",
        "        elif model_type == \"linear\":\n",
        "            # LinearExplainer for Logistic Regression\n",
        "            explainer = shap.LinearExplainer(model, X_background)\n",
        "            shap_values = explainer.shap_values(X_explain)\n",
        "\n",
        "        elif model_type == \"nn\":\n",
        "            # KernelExplainer for Neural Networks (works for ANY model)\n",
        "            def predict_fn(x):\n",
        "                \"\"\"Prediction function that handles batch processing\"\"\"\n",
        "                if len(x.shape) == 1:\n",
        "                    x = x.reshape(1, -1)\n",
        "                predictions = model.predict(x, verbose=0).flatten()\n",
        "                return predictions\n",
        "\n",
        "            print(f\"  Using KernelExplainer for neural network...\")\n",
        "            explainer = shap.KernelExplainer(predict_fn, X_background)\n",
        "            shap_values = explainer.shap_values(X_explain, nsamples=100)  # Reduced samples for speed\n",
        "\n",
        "        # Calculate feature importance from SHAP values\n",
        "        if shap_values is not None:\n",
        "            feature_importance = np.abs(shap_values).mean(0)\n",
        "            importance_df = pd.DataFrame({\n",
        "                'feature': feature_names,\n",
        "                'shap_importance': feature_importance\n",
        "            }).sort_values('shap_importance', ascending=False)\n",
        "\n",
        "            print(f\"   SHAP completed. Top feature: {importance_df.iloc[0]['feature']}\")\n",
        "            return importance_df, shap_values, X_explain\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   SHAP failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# LIME ANALYSIS (WORKS FOR ALL MODEL TYPES)\n",
        "# =============================================\n",
        "\n",
        "def perform_lime_analysis(model, X, feature_names, model_type=\"nn\", model_name=\"Model\"):\n",
        "    \"\"\"Perform LIME analysis for ANY model type\"\"\"\n",
        "    print(f\"\\n LIME Analysis - {model_name}\")\n",
        "\n",
        "    try:\n",
        "        # Create prediction function\n",
        "        if model_type == \"nn\":\n",
        "            def predict_fn(x):\n",
        "                if len(x.shape) == 1:\n",
        "                    x = x.reshape(1, -1)\n",
        "                return model.predict(x, verbose=0).flatten()\n",
        "\n",
        "            def predict_proba_fn(x):\n",
        "                preds = predict_fn(x)\n",
        "                return np.column_stack([1 - preds, preds])\n",
        "        else:\n",
        "            def predict_proba_fn(x):\n",
        "                return model.predict_proba(x)\n",
        "\n",
        "        # Create LIME explainer\n",
        "        explainer = lime_tabular.LimeTabularExplainer(\n",
        "            X[:100],  # Training data sample\n",
        "            feature_names=feature_names,\n",
        "            class_names=['No Death', 'Death'],\n",
        "            mode='classification'\n",
        "        )\n",
        "\n",
        "        # Explain multiple instances and aggregate\n",
        "        n_instances = min(10, len(X))  # Explain up to 10 instances\n",
        "        lime_importances = np.zeros(len(feature_names))\n",
        "\n",
        "        for i in range(n_instances):\n",
        "            explanation = explainer.explain_instance(\n",
        "                X[i],\n",
        "                predict_proba_fn,\n",
        "                num_features=len(feature_names)\n",
        "            )\n",
        "\n",
        "            # Extract feature importances\n",
        "            for feature_idx, importance in explanation.as_list():\n",
        "                # Find the feature index by name\n",
        "                try:\n",
        "                    feat_idx = feature_names.index(feature_idx.split('=')[0].strip())\n",
        "                    lime_importances[feat_idx] += abs(importance)\n",
        "                except:\n",
        "                    # Handle categorical features or different naming\n",
        "                    for j, fname in enumerate(feature_names):\n",
        "                        if fname in feature_idx:\n",
        "                            lime_importances[j] += abs(importance)\n",
        "                            break\n",
        "\n",
        "        # Average the importances\n",
        "        lime_importances = lime_importances / n_instances\n",
        "\n",
        "        # Create importance dataframe\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'lime_importance': lime_importances\n",
        "        }).sort_values('lime_importance', ascending=False)\n",
        "\n",
        "        print(f\"   LIME completed. Top feature: {importance_df.iloc[0]['feature']}\")\n",
        "        return importance_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   LIME failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# RUN COMPREHENSIVE ANALYSIS FOR ALL MODELS\n",
        "# =============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 1: RUNNING ALL XAI METHODS FOR ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Storage for results\n",
        "all_permutation_results = {}\n",
        "all_shap_results = {}\n",
        "all_lime_results = {}\n",
        "shap_values_dict = {}\n",
        "\n",
        "# Run analysis for each model\n",
        "for model_key, model_info in all_models.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ANALYZING: {model_info['name'].upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # 1. Permutation Importance (for all models)\n",
        "    perm_result = calculate_permutation_importance(\n",
        "        model_info['model'],\n",
        "        x_test,\n",
        "        y_test,\n",
        "        feature_names,\n",
        "        model_info['type'],\n",
        "        model_info['name']\n",
        "    )\n",
        "    if perm_result is not None:\n",
        "        all_permutation_results[model_key] = perm_result\n",
        "\n",
        "    # 2. SHAP Analysis (for all models)\n",
        "    shap_result = perform_universal_shap_analysis(\n",
        "        model_info['model'],\n",
        "        x_test,\n",
        "        feature_names,\n",
        "        model_info['type'],\n",
        "        model_info['name']\n",
        "    )\n",
        "    if shap_result is not None:\n",
        "        shap_df, shap_vals, X_sample = shap_result\n",
        "        all_shap_results[model_key] = shap_df\n",
        "        shap_values_dict[model_key] = {'values': shap_vals, 'data': X_sample}\n",
        "\n",
        "    # 3. LIME Analysis (for all models)\n",
        "    lime_result = perform_lime_analysis(\n",
        "        model_info['model'],\n",
        "        x_test,\n",
        "        feature_names,\n",
        "        model_info['type'],\n",
        "        model_info['name']\n",
        "    )\n",
        "    if lime_result is not None:\n",
        "        all_lime_results[model_key] = lime_result\n",
        "\n",
        "# CREATE COMPREHENSIVE COMPARISON\n",
        "# =============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2: COMPREHENSIVE CROSS-METHOD COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create master comparison dataframe\n",
        "comparison_data = {'feature': feature_names}\n",
        "\n",
        "# Add permutation importance\n",
        "for model_key, importance_df in all_permutation_results.items():\n",
        "    model_name = model_key.replace('_', ' ').title()\n",
        "    comparison_data[f'{model_key}_perm'] = [\n",
        "        importance_df[importance_df['feature'] == f]['importance'].iloc[0]\n",
        "        if f in importance_df['feature'].values else 0\n",
        "        for f in feature_names\n",
        "    ]\n",
        "\n",
        "# Add SHAP importance\n",
        "for model_key, shap_df in all_shap_results.items():\n",
        "    comparison_data[f'{model_key}_shap'] = [\n",
        "        shap_df[shap_df['feature'] == f]['shap_importance'].iloc[0]\n",
        "        if f in shap_df['feature'].values else 0\n",
        "        for f in feature_names\n",
        "    ]\n",
        "\n",
        "# Add LIME importance\n",
        "for model_key, lime_df in all_lime_results.items():\n",
        "    comparison_data[f'{model_key}_lime'] = [\n",
        "        lime_df[lime_df['feature'] == f]['lime_importance'].iloc[0]\n",
        "        if f in lime_df['feature'].values else 0\n",
        "        for f in feature_names\n",
        "    ]\n",
        "\n",
        "# Create the master dataframe\n",
        "master_comparison = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Calculate averages across methods\n",
        "perm_cols = [col for col in master_comparison.columns if col.endswith('_perm')]\n",
        "shap_cols = [col for col in master_comparison.columns if col.endswith('_shap')]\n",
        "lime_cols = [col for col in master_comparison.columns if col.endswith('_lime')]\n",
        "\n",
        "if perm_cols:\n",
        "    master_comparison['avg_permutation'] = master_comparison[perm_cols].mean(axis=1)\n",
        "if shap_cols:\n",
        "    master_comparison['avg_shap'] = master_comparison[shap_cols].mean(axis=1)\n",
        "if lime_cols:\n",
        "    master_comparison['avg_lime'] = master_comparison[lime_cols].mean(axis=1)\n",
        "\n",
        "# Overall average across all methods\n",
        "method_avg_cols = [col for col in ['avg_permutation', 'avg_shap', 'avg_lime']\n",
        "                   if col in master_comparison.columns]\n",
        "if method_avg_cols:\n",
        "    master_comparison['overall_average'] = master_comparison[method_avg_cols].mean(axis=1)\n",
        "\n",
        "# Sort by overall average\n",
        "if 'overall_average' in master_comparison.columns:\n",
        "    master_comparison = master_comparison.sort_values('overall_average', ascending=False)\n",
        "else:\n",
        "    master_comparison = master_comparison.sort_values('avg_permutation', ascending=False)\n",
        "\n",
        "# RESULTS SUMMARY\n",
        "# =============================================\n",
        "\n",
        "print(\"\\n COMPREHENSIVE RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n Successfully completed analysis:\")\n",
        "print(f\"    Permutation Importance: {len(all_permutation_results)}/{len(all_models)} models\")\n",
        "print(f\"    SHAP Analysis: {len(all_shap_results)}/{len(all_models)} models\")\n",
        "print(f\"    LIME Analysis: {len(all_lime_results)}/{len(all_models)} models\")\n",
        "\n",
        "print(f\"\\n TOP 10 FEATURES - MULTI-METHOD CONSENSUS:\")\n",
        "display_cols = ['feature']\n",
        "if method_avg_cols:\n",
        "    display_cols.extend(method_avg_cols)\n",
        "if 'overall_average' in master_comparison.columns:\n",
        "    display_cols.append('overall_average')\n",
        "\n",
        "print(master_comparison[display_cols].head(10).round(4).to_string(index=False))\n",
        "\n",
        "# Method Agreement Analysis\n",
        "if len(method_avg_cols) > 1:\n",
        "    print(f\"\\n  METHOD AGREEMENT ANALYSIS:\")\n",
        "    method_corr = master_comparison[method_avg_cols].corr()\n",
        "    print(\"Correlation between XAI methods:\")\n",
        "    print(method_corr.round(3).to_string())\n",
        "\n",
        "# Top feature by each method\n",
        "print(f\"\\n TOP FEATURE BY EACH METHOD:\")\n",
        "for method_col in method_avg_cols:\n",
        "    top_feature = master_comparison.loc[master_comparison[method_col].idxmax(), 'feature']\n",
        "    top_score = master_comparison[method_col].max()\n",
        "    method_name = method_col.replace('avg_', '').title()\n",
        "    print(f\"    {method_name}: '{top_feature}' (Score: {top_score:.4f})\")\n",
        "\n",
        "# VISUALIZATIONS\n",
        "# =============================================\n",
        "\n",
        "print(f\"\\n GENERATING VISUALIZATIONS...\")\n",
        "\n",
        "# Plot 1: Method Comparison Heatmap\n",
        "if len(method_avg_cols) > 1:\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_features = master_comparison.head(10)\n",
        "    heatmap_data = top_features[method_avg_cols].T\n",
        "    heatmap_data.columns = top_features['feature']\n",
        "\n",
        "    import seaborn as sns\n",
        "    sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlOrRd',\n",
        "                cbar_kws={'label': 'Importance Score'})\n",
        "    plt.title('Feature Importance Across XAI Methods (Top 10 Features)')\n",
        "    plt.ylabel('XAI Methods')\n",
        "    plt.xlabel('Features')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot 2: Overall Rankings\n",
        "plt.figure(figsize=(12, 6))\n",
        "if 'overall_average' in master_comparison.columns:\n",
        "    top_10 = master_comparison.head(10)\n",
        "    plt.barh(range(len(top_10)), top_10['overall_average'], color='skyblue', alpha=0.8)\n",
        "    plt.yticks(range(len(top_10)), top_10['feature'])\n",
        "    plt.xlabel('Overall Average Importance')\n",
        "    plt.title('Top 10 Features - Multi-Method Consensus Ranking')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    # Add value labels\n",
        "    for i, v in enumerate(top_10['overall_average']):\n",
        "        plt.text(v + 0.001, i, f'{v:.3f}', va='center', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot 3: SHAP Summary Plots (if available)\n",
        "if shap_values_dict:\n",
        "    print(f\"\\n SHAP SUMMARY PLOTS:\")\n",
        "    for model_key, shap_data in list(shap_values_dict.items())[:3]:  # Show first 3\n",
        "        model_name = all_models[model_key]['name']\n",
        "        try:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            shap.summary_plot(shap_data['values'], shap_data['data'],\n",
        "                            feature_names=feature_names, show=False)\n",
        "            plt.title(f'{model_name} - SHAP Feature Importance & Impact')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(f\"   Could not create SHAP plot for {model_name}: {str(e)}\")\n",
        "\n",
        "# FINAL INSIGHTS\n",
        "# =============================================\n",
        "\n",
        "print(f\"\\n FINAL COMPREHENSIVE INSIGHTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if 'overall_average' in master_comparison.columns:\n",
        "    top_feature = master_comparison.iloc[0]['feature']\n",
        "    top_score = master_comparison.iloc[0]['overall_average']\n",
        "    print(f\"\\n UNIVERSAL CHAMPION ACROSS ALL METHODS:\")\n",
        "    print(f\"   '{top_feature}' dominates with average score: {top_score:.4f}\")\n",
        "\n",
        "print(f\"\\n MULTI-METHOD CONSENSUS (Top 5):\")\n",
        "for i, (_, row) in enumerate(master_comparison.head(5).iterrows(), 1):\n",
        "    feature = row['feature']\n",
        "    if 'overall_average' in row:\n",
        "        score = row['overall_average']\n",
        "        print(f\"   {i}. {feature} (Multi-method avg: {score:.4f})\")\n",
        "\n",
        "# Model-specific champions\n",
        "print(f\"\\n MODEL-SPECIFIC CHAMPIONS:\")\n",
        "for model_key, perm_df in all_permutation_results.items():\n",
        "    model_name = all_models[model_key]['name']\n",
        "    top_feature = perm_df.iloc[0]['feature']\n",
        "    top_score = perm_df.iloc[0]['importance']\n",
        "    print(f\"    {model_name}: '{top_feature}' ({top_score:.4f})\")\n",
        "\n",
        "print(f\"\\n SAVING COMPREHENSIVE RESULTS\")\n",
        "master_comparison.to_csv('complete_xai_analysis_results.csv', index=False)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\" COMPLETE XAI ANALYSIS FINISHED SUCCESSFULLY!\")\n",
        "print(\" All models analyzed with Permutation + SHAP + LIME\")\n",
        "print(\" Results saved to: complete_xai_analysis_results.csv\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "aVzKmrtlb9kQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}