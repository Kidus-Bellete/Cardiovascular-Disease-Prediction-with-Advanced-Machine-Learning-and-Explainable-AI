{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1TCvyyXTWB3ZVsPHdZiPp22zulrSBCWqr",
      "authorship_tag": "ABX9TyOfv2iyWJM0V2ghvMs5Lt3F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kidus-Bellete/Cardiovascular-Disease-Prediction-with-Advanced-Machine-Learning-and-Explainable-AI/blob/main/cardiovascular_XAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install shap\n",
        "#!pip install imblearn"
      ],
      "metadata": {
        "id": "FMQ4amdm1FHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6SZco3r1A9m"
      },
      "outputs": [],
      "source": [
        "# Basic data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Deep learning with TensorFlow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# Model evaluation metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "# Explainable AI\n",
        "import shap\n",
        "\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/XAI/heart_failure_clinical_records_dataset-1-1.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "rc_7Lh531ukv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "print(\"the total number of missing value in each features:\")\n",
        "print(df.isnull().sum())\n",
        "df_duplicated = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {df_duplicated}\")\n",
        "df = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "Jc247Q2E4LX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "gmIQjLx82MXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T"
      ],
      "metadata": {
        "id": "zHxDMDw-C1fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap\n",
        "plt.figure(figsize=(14, 10))\n",
        "correlation_matrix = df.corr()\n",
        "mask = np.triu(correlation_matrix)\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
        "            mask=mask, linewidths=0.5, vmin=-1, vmax=1)\n",
        "plt.title('Correlation Heatmap of Heart Failure Features', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gy624dtjC5EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation with Target Variable (DEATH_EVENT)\n",
        "plt.figure(figsize=(8, 6))\n",
        "death_correlation = correlation_matrix['DEATH_EVENT'].drop('DEATH_EVENT').sort_values(ascending=False)\n",
        "sns.barplot(x=death_correlation.values, y=death_correlation.index, hue=death_correlation.index, palette='viridis', legend=False)\n",
        "plt.title('Correlation of Features with DEATH_EVENT', fontsize=16)\n",
        "plt.xlabel('Correlation Coefficient')\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6oRjLHs5C_Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Numerical Features by Death Event\n",
        "numerical_features = ['age', 'creatinine_phosphokinase', 'ejection_fraction',\n",
        "                      'platelets', 'serum_creatinine', 'serum_sodium', 'time']\n",
        "\n",
        "fig, axes = plt.subplots(6, 3, figsize=(10, 20))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(numerical_features):\n",
        "    if i < len(axes):\n",
        "        sns.histplot(data=df, x=feature, hue='DEATH_EVENT', kde=True, bins=25,\n",
        "                    palette={0: 'green', 1: 'red'}, alpha=0.6, ax=axes[i])\n",
        "        axes[i].set_title(f'Distribution of {feature} by Death Event', fontsize=14)\n",
        "        axes[i].set_xlabel(feature, fontsize=12)\n",
        "        axes[i].set_ylabel('Count', fontsize=12)\n",
        "        axes[i].legend(['Survived', 'Died'], title='Outcome')\n",
        "        axes[i].grid(linestyle='--', alpha=0.7)\n",
        "\n",
        "# Remove any unused subplot\n",
        "for i in range(len(numerical_features), len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rDh9iNO6DFWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplots for Key Features by Death Event\n",
        "key_features = ['age', 'ejection_fraction', 'serum_creatinine', 'time', 'serum_sodium']\n",
        "\n",
        "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(key_features):\n",
        "    if i < len(axes):\n",
        "        sns.boxplot(x='DEATH_EVENT', y=feature, data=df,\n",
        "                   hue='DEATH_EVENT', palette={0: 'green', 1: 'red'},\n",
        "                   legend=False, ax=axes[i])\n",
        "        axes[i].set_title(f'Distribution of {feature} by Death Event', fontsize=14)\n",
        "        axes[i].set_xlabel('Death Event (0=No, 1=Yes)', fontsize=12)\n",
        "        axes[i].set_ylabel(feature, fontsize=12)\n",
        "        axes[i].grid(linestyle='--', alpha=0.7)\n",
        "\n",
        "# Remove any unused subplot\n",
        "for i in range(len(key_features), len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X1f-q8sQDb9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical Features Analysis\n",
        "categorical_features = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']\n",
        "\n",
        "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(categorical_features):\n",
        "    if i < len(axes):\n",
        "        # Create a cross-tabulation\n",
        "        cross_tab = pd.crosstab(df[feature], df['DEATH_EVENT'])\n",
        "        cross_tab_percentage = cross_tab.div(cross_tab.sum(axis=1), axis=0) * 100\n",
        "\n",
        "        # Plot stacked bar chart\n",
        "        cross_tab_percentage.plot(kind='bar', stacked=True,\n",
        "                                colormap='viridis', ax=axes[i], rot=0)\n",
        "        axes[i].set_title(f'Death Rate by {feature}', fontsize=14)\n",
        "        axes[i].set_xlabel(f'{feature} (0=No, 1=Yes)', fontsize=12)\n",
        "        axes[i].set_ylabel('Percentage (%)', fontsize=12)\n",
        "        axes[i].legend(['Survived', 'Died'], title='Outcome')\n",
        "        axes[i].grid(linestyle='--', alpha=0.7)\n",
        "\n",
        "        # Add percentage labels\n",
        "        for j, p in enumerate(axes[i].patches):\n",
        "            width, height = p.get_width(), p.get_height()\n",
        "            x, y = p.get_xy()\n",
        "            if height > 5:  # Only label if percentage is greater than 5%\n",
        "                axes[i].text(x + width/2, y + height/2, f'{height:.1f}%',\n",
        "                            ha='center', va='center')\n",
        "\n",
        "# Remove any unused subplot\n",
        "for i in range(len(categorical_features), len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YJN3hffpEFm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pairplot of Most Correlated Features\n",
        "most_correlated_features = ['time', 'serum_creatinine', 'ejection_fraction', 'age', 'DEATH_EVENT']\n",
        "sns.pairplot(df[most_correlated_features], hue='DEATH_EVENT',\n",
        "             palette={0: 'green', 1: 'red'}, corner=True)\n",
        "plt.suptitle('Pairplot of Most Correlated Features', y=1.02, fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4RpBfuytEZi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Age vs Time with Death Event\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(data=df, x='age', y='time', hue='DEATH_EVENT',\n",
        "               palette={0: 'green', 1: 'red'}, size='serum_creatinine',\n",
        "               sizes=(20, 200), alpha=0.7)\n",
        "plt.title('Age vs. Follow-up Time by Death Event', fontsize=16)\n",
        "plt.xlabel('Age', fontsize=14)\n",
        "plt.ylabel('Follow-up Time (days)', fontsize=14)\n",
        "plt.grid(linestyle='--', alpha=0.7)\n",
        "plt.legend(title='Death Event', labels=['Survived', 'Died'])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u-LiBn5tFdh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of all continuous variables\n",
        "continuous_variables = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets',\n",
        "                   'serum_creatinine', 'serum_sodium', 'time']\n",
        "# Standared Scaler\n",
        "scaler = StandardScaler()\n",
        "df[continuous_variables] = scaler.fit_transform(df[continuous_variables])\n",
        "\n",
        "#MnMax Scaler\n",
        "# minmax_scaler = MinMaxScaler()\n",
        "# df[continuous_variables] = minmax_scaler.fit_transform(df[continuous_variables])\n",
        "\n",
        "#Robust Scaler\n",
        "# robust_scaler = RobustScaler()\n",
        "# df[continuous_variables] = robust_scaler.fit_transform(df[continuous_variables])"
      ],
      "metadata": {
        "id": "rze43WQoHnPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "# Define features (x) and target (y)\n",
        "x = df.iloc[:, :-1].values  # All columns except the last one (DEATH_EVENT)\n",
        "y = df.iloc[:, -1].values   # The last column (DEATH_EVENT)\n",
        "\n",
        "# Check class distribution before SMOTE\n",
        "counter_before = Counter(y)\n",
        "print(f'Class distribution before SMOTE: {counter_before}')\n",
        "\n",
        "# Apply SMOTE to balance the classes in DEATH_EVENT\n",
        "oversample = SMOTE()\n",
        "x, y = oversample.fit_resample(x, y)\n",
        "\n",
        "# Check class distribution after SMOTE\n",
        "counter_after = Counter(y)\n",
        "print(f'Class distribution after SMOTE: {counter_after}')\n",
        "\n",
        "# Split the data into training (70%) , val(15%) and tset(15%)\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.30, random_state=42)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.50, random_state=42)\n",
        "\n",
        "# Print the shapes of the datasets\n",
        "print(f\"Training set shape: {x_train.shape}\")\n",
        "print(f\"Validation set shape: {x_val.shape}\")\n",
        "print(f\"Test set shape: {x_test.shape}\")"
      ],
      "metadata": {
        "id": "IOXaZve3JebH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Models (Logistic Regression, Random Forest)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                            f1_score, roc_auc_score, classification_report)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Standardize data for Logistic Regression\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_val_scaled = scaler.transform(x_val)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "def evaluate_model(model, model_name, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation function for ML models\n",
        "    \"\"\"\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Generate predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred),\n",
        "        'Recall': recall_score(y_test, y_pred),\n",
        "        'F1': f1_score(y_test, y_pred),\n",
        "        'ROC AUC': roc_auc_score(y_test, y_prob) if y_prob is not None else None\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\"{model_name} Evaluation Results\")\n",
        "    print('='*40)\n",
        "    print(classification_report(y_test, y_pred, target_names=['No Death', 'Death']))\n",
        "\n",
        "    for metric, value in metrics.items():\n",
        "        if value is not None:\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# =============================================\n",
        "# Logistic Regression\n",
        "# =============================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LOGISTIC REGRESSION MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logreg_metrics = evaluate_model(\n",
        "    logreg,\n",
        "    \"Logistic Regression\",\n",
        "    x_train_scaled, y_train,\n",
        "    x_test_scaled, y_test\n",
        ")\n",
        "\n",
        "# =============================================\n",
        "# Random Forest\n",
        "# =============================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RANDOM FOREST MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_metrics = evaluate_model(\n",
        "    rf_model,\n",
        "    \"Random Forest\",\n",
        "    x_train, y_train,  # Random Forest doesn't require scaling\n",
        "    x_test, y_test\n",
        ")\n",
        "\n",
        "# Feature importance visualization\n",
        "X = df.drop('DEATH_EVENT', axis=1)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=feature_importance.head(15), x='importance', y='feature')\n",
        "plt.title('Top 15 Feature Importances (Random Forest)')\n",
        "plt.xlabel('Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance.head(10).to_string(index=False))\n",
        "\n",
        "# =============================================\n",
        "# Model Comparison\n",
        "# =============================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_data = {\n",
        "    'Model': ['Logistic Regression', 'Random Forest'],\n",
        "    'Accuracy': [\n",
        "        logreg_metrics['Accuracy'],\n",
        "        rf_metrics['Accuracy']\n",
        "    ],\n",
        "    'Precision': [\n",
        "        logreg_metrics['Precision'],\n",
        "        rf_metrics['Precision']\n",
        "    ],\n",
        "    'Recall': [\n",
        "        logreg_metrics['Recall'],\n",
        "        rf_metrics['Recall']\n",
        "    ],\n",
        "    'F1 Score': [\n",
        "        logreg_metrics['F1'],\n",
        "        rf_metrics['F1']\n",
        "    ],\n",
        "    'ROC AUC': [\n",
        "        logreg_metrics['ROC AUC'],\n",
        "        rf_metrics['ROC AUC']\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "print(comparison_df.round(4).to_string(index=False))"
      ],
      "metadata": {
        "id": "TOwcnAnjBFH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Notebook\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Main Network Architecture\n",
        "def build_simple_nn(input_dim, architecture='medium'):\n",
        "    model = Sequential()\n",
        "\n",
        "    if architecture == 'small':\n",
        "        # Small architecture\n",
        "        model.add(Dense(32, activation='relu', input_shape=(input_dim,)))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(16, activation='relu'))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    elif architecture == 'medium':\n",
        "        # Medium architecture\n",
        "        model.add(Dense(64, activation='relu', input_shape=(input_dim,)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Dense(16, activation='relu'))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    elif architecture == 'large':\n",
        "        # Larger architecture\n",
        "        model.add(Dense(128, activation='relu', input_shape=(input_dim,)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Dense(16, activation='relu'))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile model\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['accuracy',\n",
        "                         tf.keras.metrics.AUC(name='auc'),\n",
        "                         tf.keras.metrics.Precision(name='precision'),\n",
        "                         tf.keras.metrics.Recall(name='recall')])\n",
        "    return model\n",
        "\n",
        "def build_cnn_model(input_dim):\n",
        "    \"\"\"\n",
        "    Build a 1D CNN model for tabular data\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Reshape input for 1D CNN (samples, features, 1)\n",
        "    model.add(tf.keras.layers.Reshape((input_dim, 1), input_shape=(input_dim,)))\n",
        "\n",
        "    # First Conv1D block\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(MaxPool1D(pool_size=2))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Second Conv1D block\n",
        "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(MaxPool1D(pool_size=2))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Third Conv1D block\n",
        "    model.add(Conv1D(filters=16, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Flatten and dense layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile model\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['accuracy',\n",
        "                         tf.keras.metrics.AUC(name='auc'),\n",
        "                         tf.keras.metrics.Precision(name='precision'),\n",
        "                         tf.keras.metrics.Recall(name='recall')])\n",
        "    return model\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "# Train models with different architectures\n",
        "architectures = ['small', 'medium', 'large', 'cnn']\n",
        "models = {}\n",
        "histories = {}\n",
        "weighted_models = {}  # New dictionary for weighted models\n",
        "weighted_histories = {}  # New dictionary for weighted histories\n",
        "\n",
        "for arch in architectures:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training {arch.upper()} {'CNN' if arch == 'cnn' else 'Neural Network'}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Build model\n",
        "    if arch == 'cnn':\n",
        "        model = build_cnn_model(x_train.shape[1])\n",
        "    else:\n",
        "        model = build_simple_nn(x_train.shape[1], arch)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_auc', patience=20, mode='max', restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n",
        "    ]\n",
        "\n",
        "    # Train standard model\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        validation_data=(x_val, y_val),\n",
        "        epochs=50,\n",
        "        batch_size=16,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    models[arch] = model\n",
        "    histories[arch] = history\n",
        "\n",
        "    # Train weighted version (except for CNN)\n",
        "    if arch != 'cnn':\n",
        "        print(f\"\\nTraining WEIGHTED {arch.upper()} Neural Network\")\n",
        "        weighted_model = build_simple_nn(x_train.shape[1], arch)\n",
        "\n",
        "        weighted_history = weighted_model.fit(\n",
        "            x_train, y_train,\n",
        "            validation_data=(x_val, y_val),\n",
        "            epochs=50,\n",
        "            batch_size=16,\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weight_dict,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        weighted_models[f'{arch}_weighted'] = weighted_model\n",
        "        weighted_histories[f'{arch}_weighted'] = weighted_history"
      ],
      "metadata": {
        "id": "FxhOwguiFTNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (roc_curve, roc_auc_score, classification_report,\n",
        "                            confusion_matrix, precision_recall_curve, average_precision_score)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# =============================================\n",
        "# 1. Evaluation Functions\n",
        "# =============================================\n",
        "\n",
        "def evaluate_ml_model(model, X, y, model_name, model_type=\"ML\"):\n",
        "    \"\"\"\n",
        "    Evaluate traditional ML models (Logistic Regression, SVM, Random Forest)\n",
        "    \"\"\"\n",
        "    y_prob = model.predict_proba(X)[:, 1]\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # Metrics\n",
        "    metrics = {\n",
        "        'Accuracy': accuracy_score(y, y_pred),\n",
        "        'Precision': precision_score(y, y_pred),\n",
        "        'Recall': recall_score(y, y_pred),\n",
        "        'F1': f1_score(y, y_pred),\n",
        "        'ROC AUC': roc_auc_score(y, y_prob),\n",
        "        'PR AUC': average_precision_score(y, y_prob)\n",
        "    }\n",
        "\n",
        "    # Print report\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{model_name} ({model_type}) Evaluation\")\n",
        "    print('='*60)\n",
        "    print(classification_report(y, y_pred, target_names=['No Death', 'Death']))\n",
        "\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    return metrics, y_prob, y_pred\n",
        "\n",
        "def evaluate_nn_model(model, X, y, model_name):\n",
        "    \"\"\"\n",
        "    Evaluate neural network models\n",
        "    \"\"\"\n",
        "    y_prob = model.predict(X, verbose=0).flatten()\n",
        "    y_pred = (y_prob > 0.5).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    metrics = {\n",
        "        'Accuracy': accuracy_score(y, y_pred),\n",
        "        'Precision': precision_score(y, y_pred),\n",
        "        'Recall': recall_score(y, y_pred),\n",
        "        'F1': f1_score(y, y_pred),\n",
        "        'ROC AUC': roc_auc_score(y, y_prob),\n",
        "        'PR AUC': average_precision_score(y, y_prob)\n",
        "    }\n",
        "\n",
        "    # Print report\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{model_name} (Neural Network) Evaluation\")\n",
        "    print('='*60)\n",
        "    print(classification_report(y, y_pred, target_names=['No Death', 'Death']))\n",
        "\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    return metrics, y_prob, y_pred\n",
        "\n",
        "# =============================================\n",
        "# 2. Visualization Functions\n",
        "# =============================================\n",
        "\n",
        "# def plot_confusion_matrix(y_true, y_pred, model_name):\n",
        "#     cm = confusion_matrix(y_true, y_pred)\n",
        "#     plt.figure(figsize=(6, 6))\n",
        "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "#                 xticklabels=['No Death', 'Death'],\n",
        "#                 yticklabels=['No Death', 'Death'])\n",
        "#     plt.title(f'{model_name} - Confusion Matrix')\n",
        "#     plt.ylabel('True label')\n",
        "#     plt.xlabel('Predicted label')\n",
        "#     plt.show()\n",
        "\n",
        "def plot_roc_curves(results_dict):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for name, data in results_dict.items():\n",
        "        fpr, tpr, _ = roc_curve(data['y_true'], data['y_prob'])\n",
        "        auc_score = roc_auc_score(data['y_true'], data['y_prob'])\n",
        "        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})', linewidth=2)\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves Comparison')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "def plot_pr_curves(results_dict):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for name, data in results_dict.items():\n",
        "        precision, recall, _ = precision_recall_curve(data['y_true'], data['y_prob'])\n",
        "        ap_score = average_precision_score(data['y_true'], data['y_prob'])\n",
        "        plt.plot(recall, precision, label=f'{name} (AP = {ap_score:.3f})', linewidth=2)\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curves')\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "def plot_metric_comparison(comparison_df):\n",
        "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC', 'PR AUC']\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, metric in enumerate(metrics, 1):\n",
        "        plt.subplot(2, 3, i)\n",
        "        sns.barplot(data=comparison_df, x=metric, y='Model')\n",
        "        plt.title(metric)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# =============================================\n",
        "# 3. Main Evaluation Section\n",
        "# =============================================\n",
        "\n",
        "# Initialize storage for results\n",
        "all_results = {}\n",
        "comparison_data = []\n",
        "\n",
        "# Evaluate ML Models (assuming they're trained in Notebook 1)\n",
        "ml_models = {\n",
        "    'Logistic Regression': logreg,\n",
        "    'Random Forest': rf_model\n",
        "}\n",
        "\n",
        "for name, model in ml_models.items():\n",
        "    metrics, y_prob, y_pred = evaluate_ml_model(model, x_test, y_test, name)\n",
        "    all_results[name] = {'y_true': y_test, 'y_prob': y_prob, 'y_pred': y_pred}\n",
        "    comparison_data.append({'Model': name, **metrics})\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    #plot_confusion_matrix(y_test, y_pred, name)\n",
        "\n",
        "# Evaluate Neural Networks (assuming they're trained in Notebook 2)\n",
        "nn_models = {\n",
        "    'Small NN': models['small'],\n",
        "    'Medium NN': models['medium'],\n",
        "    'Large NN': models['large'],\n",
        "    'CNN': models['cnn']\n",
        "}\n",
        "\n",
        "for name, model in nn_models.items():\n",
        "    metrics, y_prob, y_pred = evaluate_nn_model(model, x_test, y_test, name)\n",
        "    all_results[name] = {'y_true': y_test, 'y_prob': y_prob, 'y_pred': y_pred}\n",
        "    comparison_data.append({'Model': name, **metrics})\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    #plot_confusion_matrix(y_test, y_pred, name)\n",
        "\n",
        "# Evaluate Weighted Neural Networks (if available)\n",
        "if 'weighted_models' in globals():\n",
        "    weighted_nn_models = {\n",
        "        'Small NN (Weighted)': weighted_models['small_weighted'],\n",
        "        'Medium NN (Weighted)': weighted_models['medium_weighted'],\n",
        "        'Large NN (Weighted)': weighted_models['large_weighted']\n",
        "    }\n",
        "\n",
        "    for name, model in weighted_nn_models.items():\n",
        "        metrics, y_prob, y_pred = evaluate_nn_model(model, x_test, y_test, name)\n",
        "        all_results[name] = {'y_true': y_test, 'y_prob': y_prob, 'y_pred': y_pred}\n",
        "        comparison_data.append({'Model': name, **metrics})\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        #plot_confusion_matrix(y_test, y_pred, name)\n",
        "\n",
        "# =============================================\n",
        "# 4. Comprehensive Comparison\n",
        "# =============================================\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Display comparison table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.round(4).to_string(index=False))\n",
        "\n",
        "# Visual comparisons\n",
        "#plot_metric_comparison(comparison_df)\n",
        "#plot_roc_curves(all_results)\n",
        "#plot_pr_curves(all_results)\n",
        "\n",
        "# =============================================\n",
        "# 5. Neural Network Training Analysis\n",
        "# =============================================\n",
        "\n",
        "if 'histories' in globals():\n",
        "    # Plot training histories for neural networks\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "    fig.suptitle('Neural Network Training Histories', fontsize=16)\n",
        "\n",
        "    for i, (arch, history) in enumerate(histories.items()):\n",
        "        # AUC plot\n",
        "        axes[0, i].plot(history.history['auc'], label='Train AUC', alpha=0.8)\n",
        "        axes[0, i].plot(history.history['val_auc'], label='Validation AUC', alpha=0.8)\n",
        "        model_name = 'CNN' if arch == 'cnn' else f'{arch.capitalize()} NN'\n",
        "        axes[0, i].set_title(f'{model_name} - AUC')\n",
        "        axes[0, i].set_ylabel('AUC')\n",
        "        axes[0, i].set_xlabel('Epoch')\n",
        "        axes[0, i].legend()\n",
        "        axes[0, i].grid(True, alpha=0.3)\n",
        "\n",
        "        # Loss plot\n",
        "        axes[1, i].plot(history.history['loss'], label='Train Loss', alpha=0.8)\n",
        "        axes[1, i].plot(history.history['val_loss'], label='Validation Loss', alpha=0.8)\n",
        "        axes[1, i].set_title(f'{model_name} - Loss')\n",
        "        axes[1, i].set_ylabel('Loss')\n",
        "        axes[1, i].set_xlabel('Epoch')\n",
        "        axes[1, i].legend()\n",
        "        axes[1, i].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yKtDWh4FFUv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install lime"
      ],
      "metadata": {
        "id": "U81EyNWz-9cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook 4: Fixed XAI Analysis for All Models\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from sklearn.metrics import roc_auc_score\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"WORKING XAI ANALYSIS - ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. INITIALIZATION AND DATA PREP\n",
        "# =============================================\n",
        "\n",
        "# Get feature names (handling both DataFrame and numpy array cases)\n",
        "try:\n",
        "    feature_names = x_train.columns.tolist()\n",
        "except AttributeError:\n",
        "    feature_names = [f'Feature {i}' for i in range(x_train.shape[1])]\n",
        "\n",
        "# Prepare validation data (ensure it's numpy array)\n",
        "if isinstance(x_val, pd.DataFrame):\n",
        "    x_val_array = x_val.values\n",
        "else:\n",
        "    x_val_array = x_val.copy()\n",
        "\n",
        "# Define all models to analyze\n",
        "model_configs = {\n",
        "    'Random Forest': {'model': rf_model, 'type': 'tree'},\n",
        "    'Logistic Reg': {'model': logreg, 'type': 'linear'}\n",
        "}\n",
        "\n",
        "# For neural networks we'll use a different approach\n",
        "nn_configs = {\n",
        "    'Small NN': models['small'],\n",
        "    'Medium NN': models['medium'],\n",
        "    'Large NN': models['large'],\n",
        "    'CNN': models['cnn']\n",
        "}\n",
        "\n",
        "# 2. PERMUTATION IMPORTANCE ANALYSIS\n",
        "# =============================================\n",
        "\n",
        "print(\"\\nSTEP 1: RELIABLE PERMUTATION IMPORTANCE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def manual_permutation_importance(model, X, y, model_type, n_repeats=5):\n",
        "    \"\"\"Custom permutation importance calculation that works for all model types\"\"\"\n",
        "    try:\n",
        "        # Baseline score\n",
        "        if model_type == 'nn':\n",
        "            pred_fn = lambda x: model.predict(x, verbose=0).flatten()\n",
        "        else:\n",
        "            pred_fn = lambda x: model.predict_proba(x)[:, 1]\n",
        "\n",
        "        baseline_score = roc_auc_score(y, pred_fn(X))\n",
        "\n",
        "        # Calculate importance for each feature\n",
        "        importances = np.zeros(X.shape[1])\n",
        "        for i in range(X.shape[1]):\n",
        "            scores = []\n",
        "            for _ in range(n_repeats):\n",
        "                X_permuted = X.copy()\n",
        "                np.random.shuffle(X_permuted[:, i])\n",
        "                permuted_score = roc_auc_score(y, pred_fn(X_permuted))\n",
        "                scores.append(baseline_score - permuted_score)\n",
        "            importances[i] = np.mean(scores)\n",
        "\n",
        "        return importances\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating importance: {str(e)}\")\n",
        "        return np.zeros(X.shape[1])\n",
        "\n",
        "perm_results = {}\n",
        "for name, config in tqdm(model_configs.items(), desc=\"Calculating Permutation Importance\"):\n",
        "    importances = manual_permutation_importance(\n",
        "        config['model'],\n",
        "        x_val_array,\n",
        "        y_val,\n",
        "        config['type']\n",
        "    )\n",
        "    perm_results[name] = importances\n",
        "\n",
        "# For neural networks (slower but reliable)\n",
        "print(\"\\nCalculating for Neural Networks (this may take a while)...\")\n",
        "for name, model in tqdm(nn_configs.items(), desc=\"Neural Networks\"):\n",
        "    importances = manual_permutation_importance(\n",
        "        model,\n",
        "        x_val_array,\n",
        "        y_val,\n",
        "        'nn'\n",
        "    )\n",
        "    perm_results[name] = importances\n",
        "\n",
        "# Create permutation importance dataframe\n",
        "perm_df = pd.DataFrame(perm_results, index=feature_names)\n",
        "perm_df['Average'] = perm_df.mean(axis=1)\n",
        "perm_df = perm_df.sort_values('Average', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Features by Average Permutation Importance:\")\n",
        "print(perm_df.head(10).round(4))\n",
        "\n",
        "# 3. FEATURE IMPORTANCE VISUALIZATION (FIXED)\n",
        "# =============================================\n",
        "\n",
        "print(\"\\nSTEP 2: FEATURE IMPORTANCE VISUALIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Fix 1: Heatmap of top features\n",
        "plt.figure(figsize=(12, 8))\n",
        "# Remove 'Average' column for heatmap\n",
        "top_features = perm_df.head(15).drop('Average', axis=1)\n",
        "sns.heatmap(top_features.T,\n",
        "            annot=True, fmt=\".3f\",\n",
        "            cmap=\"YlOrRd\",\n",
        "            cbar_kws={'label': 'Importance Score'})\n",
        "plt.title(\"Top 15 Features - Importance Across Models\", fontsize=14, pad=20)\n",
        "plt.xlabel(\"Features\", fontsize=12)\n",
        "plt.ylabel(\"Models\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Fix 2: Line plot for top 5 features across models\n",
        "top_5_features = perm_df.index[:5]\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Get model names (excluding 'Average')\n",
        "model_names = [col for col in perm_df.columns if col != 'Average']\n",
        "\n",
        "for feature in top_5_features:\n",
        "    # Get values for each model (excluding 'Average')\n",
        "    values = [perm_df.loc[feature, model] for model in model_names]\n",
        "    plt.plot(model_names, values, 'o-', label=feature, linewidth=2, markersize=6)\n",
        "\n",
        "plt.title(\"Importance Scores for Top 5 Features Across Models\", fontsize=14, pad=20)\n",
        "plt.ylabel(\"Importance Score\", fontsize=12)\n",
        "plt.xlabel(\"Models\", fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Additional Visualization: Bar plot of average importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_10_avg = perm_df.head(10)['Average']\n",
        "bars = plt.bar(range(len(top_10_avg)), top_10_avg.values,\n",
        "               color='skyblue', edgecolor='navy', alpha=0.7)\n",
        "plt.title(\"Top 10 Features by Average Importance\", fontsize=14, pad=20)\n",
        "plt.xlabel(\"Features\", fontsize=12)\n",
        "plt.ylabel(\"Average Importance Score\", fontsize=12)\n",
        "plt.xticks(range(len(top_10_avg)), top_10_avg.index, rotation=45)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, bar in enumerate(bars):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
        "             f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. MODEL COMPARISON ANALYSIS\n",
        "# =============================================\n",
        "\n",
        "print(\"\\nSTEP 3: MODEL COMPARISON ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create a comprehensive comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Plot 1: Feature importance correlation between models\n",
        "corr_matrix = perm_df.drop('Average', axis=1).corr()\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
        "            center=0, ax=axes[0,0])\n",
        "axes[0,0].set_title(\"Model Agreement on Feature Importance\")\n",
        "\n",
        "# Plot 2: Distribution of importance scores by model\n",
        "model_data = []\n",
        "for model in model_names:\n",
        "    for importance in perm_df[model]:\n",
        "        model_data.append({'Model': model, 'Importance': importance})\n",
        "model_comparison_df = pd.DataFrame(model_data)\n",
        "\n",
        "sns.boxplot(data=model_comparison_df, x='Model', y='Importance', ax=axes[0,1])\n",
        "axes[0,1].set_title(\"Distribution of Importance Scores by Model\")\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Plot 3: Top features by each model\n",
        "top_by_model = {}\n",
        "for model in model_names:\n",
        "    top_by_model[model] = perm_df.nlargest(5, model).index.tolist()\n",
        "\n",
        "# Create a stacked bar chart showing how often each feature appears in top 5\n",
        "feature_counts = {}\n",
        "for features in top_by_model.values():\n",
        "    for feature in features:\n",
        "        feature_counts[feature] = feature_counts.get(feature, 0) + 1\n",
        "\n",
        "sorted_features = sorted(feature_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "features, counts = zip(*sorted_features)\n",
        "\n",
        "axes[1,0].bar(range(len(features)), counts, color='lightcoral', alpha=0.7)\n",
        "axes[1,0].set_title(\"Features Most Often in Top 5 (Across Models)\")\n",
        "axes[1,0].set_xlabel(\"Features\")\n",
        "axes[1,0].set_ylabel(\"Times in Top 5\")\n",
        "axes[1,0].set_xticks(range(len(features)))\n",
        "axes[1,0].set_xticklabels(features, rotation=45)\n",
        "\n",
        "# Plot 4: Stability vs Importance scatter\n",
        "stability = perm_df.drop('Average', axis=1).std(axis=1)\n",
        "importance = perm_df['Average']\n",
        "\n",
        "scatter = axes[1,1].scatter(importance, stability, alpha=0.6, s=60)\n",
        "axes[1,1].set_xlabel(\"Average Importance\")\n",
        "axes[1,1].set_ylabel(\"Importance Stability (Std Dev)\")\n",
        "axes[1,1].set_title(\"Feature Importance vs Stability\")\n",
        "\n",
        "# Annotate most important and most stable features\n",
        "for i, txt in enumerate(perm_df.index[:5]):  # Top 5 by importance\n",
        "    axes[1,1].annotate(txt, (importance.iloc[i], stability.iloc[i]),\n",
        "                      xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5. KEY INSIGHTS AND RECOMMENDATIONS\n",
        "# =============================================\n",
        "\n",
        "print(\"\\nSTEP 4: KEY INSIGHTS AND RECOMMENDATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Identify consensus important features\n",
        "threshold = perm_df['Average'].quantile(0.75)\n",
        "consensus_features = perm_df[perm_df['Average'] > threshold].index.tolist()\n",
        "\n",
        "print(\"\\n1. CONSENSUS IMPORTANT FEATURES (Across All Models):\")\n",
        "for i, feat in enumerate(consensus_features[:5], 1):\n",
        "    print(f\"  {i}. {feat} (Avg Importance: {perm_df.loc[feat, 'Average']:.3f})\")\n",
        "\n",
        "# Identify model-specific important features\n",
        "print(\"\\n2. MODEL-SPECIFIC IMPORTANT FEATURES:\")\n",
        "for model in model_names:\n",
        "    top_feat = perm_df[model].idxmax()\n",
        "    print(f\"  - {model}: {top_feat} (Score: {perm_df.loc[top_feat, model]:.3f})\")\n",
        "\n",
        "# Stability analysis\n",
        "print(\"\\n3. FEATURE STABILITY ANALYSIS:\")\n",
        "stability_scores = perm_df.drop('Average', axis=1).std(axis=1)\n",
        "stable_features = stability_scores.sort_values().index[:3]\n",
        "volatile_features = stability_scores.sort_values(ascending=False).index[:3]\n",
        "\n",
        "print(\"  Most Stable Features (consistent across models):\")\n",
        "for feat in stable_features:\n",
        "    print(f\"    {feat} (Std: {stability_scores[feat]:.3f})\")\n",
        "\n",
        "print(\"\\n  Most Volatile Features (model-dependent importance):\")\n",
        "for feat in volatile_features:\n",
        "    print(f\"    {feat} (Std: {stability_scores[feat]:.3f})\")\n",
        "\n",
        "# Model agreement analysis\n",
        "print(\"\\n4. MODEL AGREEMENT ANALYSIS:\")\n",
        "correlation_scores = corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)]\n",
        "print(f\"  Average correlation between models: {np.mean(correlation_scores):.3f}\")\n",
        "print(f\"  Models agree most: {corr_matrix.max().max():.3f}\")\n",
        "print(f\"  Models agree least: {corr_matrix.min().min():.3f}\")\n",
        "\n",
        "# Final recommendations\n",
        "print(\"\\n5. RECOMMENDATIONS:\")\n",
        "print(\"  • Focus on consensus features for robust model interpretability\")\n",
        "print(\"  • Investigate model-specific important features for specialized insights\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ENHANCED XAI ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save comprehensive results\n",
        "results_summary = {\n",
        "    'feature_importance': perm_df,\n",
        "    'model_correlations': corr_matrix,\n",
        "    'stability_scores': stability_scores,\n",
        "    'consensus_features': consensus_features,\n",
        "    'top_by_model': top_by_model\n",
        "}\n",
        "\n",
        "# Save main results\n",
        "perm_df.to_csv(\"feature_importance_results.csv\")\n",
        "corr_matrix.to_csv(\"model_correlation_matrix.csv\")\n",
        "\n",
        "print(\"\\nResults saved:\")\n",
        "print(\"- feature_importance_results.csv\")\n",
        "print(\"- model_correlation_matrix.csv\")"
      ],
      "metadata": {
        "id": "vpwKCJzkeVIj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}